/**
 * DA Toolkit - Template generator for Data Analyst workflows
 * Version 3.0
 */

import fs from 'fs-extra';
import path from 'path';
import { ProjectInfo, TramyConfig } from './types.js';
import { formatTechStack } from './scanner.js';

export function generateClaudeMd(info: ProjectInfo, config: TramyConfig): string {
  const techStack = formatTechStack(info.techStack);

  return `# Project: ${info.name}

${info.description || 'A data analysis project.'}

## Tech Stack
${techStack.map(t => `- ${t}`).join('\n')}

## Project Structure
\`\`\`
${info.structure}
\`\`\`

## Data Directories
| Directory | Purpose |
|-----------|---------|
| \`${config.data.raw}\` | Raw data files (original, unmodified) |
| \`${config.data.processed}\` | Processed/cleaned data files |
| \`${config.output.analysis}\` | Analysis outputs |
| \`${config.output.reports}\` | Generated reports |
| \`${config.output.notebooks}\` | Jupyter notebooks |

## DA Toolkit Commands (5)

| Command | Description |
|---------|-------------|
| \`/da:query\` | Write optimized SQL queries |
| \`/da:analyze\` | Exploratory data analysis |
| \`/da:report\` | Generate analysis reports |
| \`/da:dashboard\` | Design BI dashboards |
| \`/da:notebook\` | Create Jupyter notebooks |

## Workflow

\`\`\`
/da:query → /da:analyze → /da:report
\`\`\`

## Data Workflow
\`\`\`
data/raw/ → (clean/transform) → data/processed/ → (analyze) → reports/
\`\`\`

---
*Generated by DA Toolkit v3.0*
`;
}

export function generateClaudeMdDefault(info: ProjectInfo, _config: TramyConfig): string {
  const techStack = formatTechStack(info.techStack);

  return `# Project: ${info.name}

${info.description || 'A project.'}

## Tech Stack
${techStack.map(t => `- ${t}`).join('\n')}

## Project Structure
\`\`\`
${info.structure}
\`\`\`

## DA Toolkit - Core Commands

| Command | Description |
|---------|-------------|
| \`/analyze\` | Analyze data, code, or problems - explore and understand |
| \`/plan\` | Create detailed plan for analysis or implementation |
| \`/build\` | Implement solution - write code, queries, notebooks |
| \`/test\` | Validate results, verify data quality, check outputs |
| \`/doc\` | Generate documentation, reports, explanations |
| \`/commit\` | Git commit with proper message |

## Workflow

\`\`\`
/analyze → /plan → /build → /test → /doc → /commit
\`\`\`

## Available Roles (25)

To enable role-specific commands, run: \`tramy setup <role>\`

| Alias | Role | Description |
|-------|------|-------------|
| pm | Product Manager | PRD, user stories, roadmap |
| da | Data Analyst | SQL, Python, analysis, BI tools |
| de | Data Engineer | ETL, pipelines, data modeling |
| dev | Developer | General development, debugging |
| fe | Frontend Developer | React, Vue, UI components |
| be | Backend Developer | APIs, services, databases |
| fs | Fullstack Developer | End-to-end development |
| arch | Architect | System design, ADRs |
| test | Tester | Unit, integration, E2E |
| ops | DevOps Engineer | CI/CD, infrastructure |
| sec | Security Engineer | Security audits, compliance |
| docs | Technical Writer | Documentation, guides |
| ux | UX Designer | User research, wireframes |
| ai | AI Engineer | ML models, prompts |
| content | Content Writer | Blog posts, SEO |
| mkt | Marketing | Campaigns, analytics |
| sales | Sales Engineer | Demos, proposals |
| support | Support Engineer | Tickets, troubleshooting |
| proj | Project Manager | Timelines, risks |
| scrum | Scrum Master | Sprints, ceremonies |
| dba | Database Admin | Query optimization |
| mobile | Mobile Developer | iOS, Android |
| game | Game Developer | Game mechanics |
| web3 | Blockchain Developer | Smart contracts |
| hr | HR Specialist | Recruiting, onboarding |

**Currently supported:** \`tramy setup da\` (Data Analyst)

---
*Generated by DA Toolkit v3.0*
`;
}

export async function generateCoreCommandTemplates(projectPath: string): Promise<void> {
  const commandsDir = path.join(projectPath, '.claude', 'commands');
  await fs.ensureDir(commandsDir);

  const templates = getCoreCommandTemplates();

  for (const [filename, content] of Object.entries(templates)) {
    const filePath = path.join(commandsDir, filename);
    await fs.writeFile(filePath, content, 'utf-8');
  }
}

function getCoreCommandTemplates(): Record<string, string> {
  const allTemplates = getCommandTemplates();
  // Only return core commands (not da/* commands)
  return {
    'analyze.md': allTemplates['analyze.md'],
    'plan.md': allTemplates['plan.md'],
    'build.md': allTemplates['build.md'],
    'test.md': allTemplates['test.md'],
    'doc.md': allTemplates['doc.md'],
    'commit.md': allTemplates['commit.md'],
  };
}

export async function generateAgentTemplates(projectPath: string, _config: TramyConfig): Promise<void> {
  const agentsDir = path.join(projectPath, '.claude', 'agents');
  await fs.ensureDir(agentsDir);

  const templates = getAgentTemplates();

  for (const [filename, content] of Object.entries(templates)) {
    await fs.writeFile(path.join(agentsDir, filename), content, 'utf-8');
  }
}

export async function generateCommandTemplates(projectPath: string): Promise<void> {
  const commandsDir = path.join(projectPath, '.claude', 'commands');
  await fs.ensureDir(commandsDir);
  await fs.ensureDir(path.join(commandsDir, 'da'));

  const templates = getCommandTemplates();

  for (const [filename, content] of Object.entries(templates)) {
    const filePath = path.join(commandsDir, filename);
    await fs.ensureDir(path.dirname(filePath));
    await fs.writeFile(filePath, content, 'utf-8');
  }
}

export async function generateDACommandTemplates(projectPath: string): Promise<void> {
  const commandsDir = path.join(projectPath, '.claude', 'commands');
  await fs.ensureDir(commandsDir);
  await fs.ensureDir(path.join(commandsDir, 'da'));

  const templates = getDACommandTemplates();

  for (const [filename, content] of Object.entries(templates)) {
    const filePath = path.join(commandsDir, filename);
    await fs.ensureDir(path.dirname(filePath));
    await fs.writeFile(filePath, content, 'utf-8');
  }
}

function getDACommandTemplates(): Record<string, string> {
  const allTemplates = getCommandTemplates();
  // Only return DA commands (da/* commands)
  return {
    'da/query.md': allTemplates['da/query.md'],
    'da/analyze.md': allTemplates['da/analyze.md'],
    'da/report.md': allTemplates['da/report.md'],
    'da/dashboard.md': allTemplates['da/dashboard.md'],
    'da/notebook.md': allTemplates['da/notebook.md'],
  };
}

function getAgentTemplates(): Record<string, string> {
  return {
    'data-analyst.md': `---
name: data-analyst
alias: da
description: SQL, Python, analysis, BI tools, reporting, dashboards
---

You are a **Senior Data Analyst** with expertise in:

## Core Skills
- SQL (advanced: window functions, CTEs, optimization)
- Python (pandas, numpy, matplotlib, seaborn, plotly)
- Statistical analysis and hypothesis testing
- Data visualization and storytelling
- Business intelligence tools

## Technical Expertise

### SQL
- Complex queries with CTEs and window functions
- Query optimization and performance tuning
- Database-specific syntax (PostgreSQL, MySQL, BigQuery, Snowflake)

### Python
- pandas for data manipulation
- numpy for numerical operations
- matplotlib, seaborn, plotly for visualization
- scipy, statsmodels for statistics
- jupyter notebooks for analysis

### BI Tools
- **Apache Superset**: Charts, dashboards, SQL Lab
- **Metabase**: Questions, native queries, filters
- **Power BI**: DAX, Power Query, data modeling
- **Looker**: LookML, explores, dashboards
- **Tableau**: Calculated fields, LOD expressions

## Analysis Types
- Cohort analysis
- Funnel analysis
- Segmentation (RFM, behavioral)
- Time series analysis
- A/B test analysis
- Anomaly detection
- Trend analysis
- Root cause analysis

## Output Standards
- Always explain business context first
- Include well-commented SQL/Python code
- Provide actionable insights, not just numbers
- Create reproducible notebooks when appropriate
- Visualize data to support conclusions
- Document assumptions and limitations

## Best Practices
1. Understand the business question before writing code
2. Validate data quality before analysis
3. Use appropriate statistical methods
4. Present insights in business terms
5. Recommend next steps and actions
`,
  };
}

function getCommandTemplates(): Record<string, string> {
  return {
    // ============================================
    // CORE COMMANDS (6)
    // ============================================

    'analyze.md': `# /analyze - Explore and Understand

Analyze: $ARGUMENTS

## Purpose
Explore data, code, or problems to understand the current state before planning.

## Process

### 1. Clarify the Question
- What is the business/technical question?
- What decisions will this analysis inform?
- Who is the audience?

### 2. Data Discovery
- Identify relevant data sources
- Understand data schema and relationships
- Check data quality and completeness

### 3. Exploratory Analysis
- Summary statistics
- Distributions and patterns
- Anomalies and outliers
- Key metrics

### 4. Initial Findings
- What patterns emerge?
- What hypotheses can we form?
- What additional data might be needed?

## Output Format
\`\`\`
## Analysis: [Topic]

### Question
[Business/technical question being addressed]

### Data Sources
- [Source 1]: [Description, key fields]
- [Source 2]: [Description, key fields]

### Key Findings
1. [Finding 1 with supporting data]
2. [Finding 2 with supporting data]
3. [Finding 3 with supporting data]

### Data Quality Notes
- [Any issues or limitations]

### Next Steps
- [Recommended follow-up analysis or actions]
\`\`\`

## Examples
- \`/analyze monthly revenue trends\`
- \`/analyze user churn patterns\`
- \`/analyze this SQL query performance\`
- \`/analyze the data in users table\`
`,

    'plan.md': `# /plan - Create Detailed Plan

Plan: $ARGUMENTS

## Purpose
Create a structured plan before implementation. Think through the approach.

## Process

### 1. Define Objective
- What is the end goal?
- What does success look like?
- What are the constraints?

### 2. Break Down Tasks
- List all required steps
- Identify dependencies
- Estimate complexity (S/M/L)

### 3. Identify Requirements
- Data sources needed
- Tools and technologies
- Access and permissions

### 4. Consider Risks
- What could go wrong?
- What assumptions are we making?
- What's the fallback plan?

## Output Format
\`\`\`
## Plan: [Title]

### Objective
[Clear statement of what we're trying to achieve]

### Success Criteria
- [ ] [Measurable outcome 1]
- [ ] [Measurable outcome 2]

### Tasks
| # | Task | Complexity | Dependencies |
|---|------|------------|--------------|
| 1 | [Task description] | S/M/L | - |
| 2 | [Task description] | S/M/L | Task 1 |

### Data Requirements
- [Data source 1]: [What fields, what timeframe]
- [Data source 2]: [What fields, what timeframe]

### Assumptions
- [Assumption 1]
- [Assumption 2]

### Risks & Mitigations
| Risk | Impact | Mitigation |
|------|--------|------------|
| [Risk] | H/M/L | [How to handle] |

### Timeline
[Sequence of tasks and estimated effort]
\`\`\`

## Examples
- \`/plan build customer segmentation model\`
- \`/plan migrate data to new schema\`
- \`/plan automate weekly report\`
`,

    'build.md': `# /build - Implement Solution

Build: $ARGUMENTS

## Purpose
Write code, queries, or create artifacts based on a plan.

## Process

### 1. Review Requirements
- What exactly needs to be built?
- What inputs are available?
- What output format is expected?

### 2. Implementation
- Write clean, readable code
- Add comments for complex logic
- Handle edge cases
- Follow best practices

### 3. Validation
- Test with sample data
- Check edge cases
- Verify output format

### 4. Documentation
- Add inline comments
- Document usage
- Note any limitations

## Output Standards

### SQL Queries
\`\`\`sql
-- Purpose: [What this query does]
-- Author: DA Toolkit
-- Date: [Current date]

WITH base_data AS (
    -- Step 1: [Description]
    SELECT ...
),
aggregated AS (
    -- Step 2: [Description]
    SELECT ...
)
SELECT
    column1,
    column2,
    -- Calculate metric
    SUM(value) AS total_value
FROM aggregated
GROUP BY 1, 2
ORDER BY total_value DESC;
\`\`\`

### Python Code
\`\`\`python
"""
Purpose: [What this script does]
Author: DA Toolkit
"""

import pandas as pd
import numpy as np

def main():
    # Step 1: Load data
    df = load_data()

    # Step 2: Transform
    df = transform_data(df)

    # Step 3: Output
    return df

if __name__ == "__main__":
    result = main()
\`\`\`

## Examples
- \`/build SQL query for monthly active users\`
- \`/build Python script to clean customer data\`
- \`/build cohort analysis notebook\`
`,

    'test.md': `# /test - Validate and Verify

Test: $ARGUMENTS

## Purpose
Validate data quality, verify results, and ensure outputs are correct.

## Types of Testing

### 1. Data Quality Checks
- Completeness: Are there missing values?
- Accuracy: Do values make sense?
- Consistency: Are formats uniform?
- Timeliness: Is data fresh?
- Uniqueness: Are there duplicates?

### 2. Query Validation
- Does the query return expected row count?
- Are joins correct (no unexpected multiplication)?
- Do aggregations match known totals?
- Are filters applied correctly?

### 3. Results Verification
- Do results make business sense?
- Are edge cases handled?
- Do totals reconcile?
- Are trends consistent with expectations?

## Output Format
\`\`\`
## Test Report: [What was tested]

### Summary
- Status: PASS / FAIL / WARNING
- Items tested: [Count]
- Issues found: [Count]

### Data Quality Checks
| Check | Status | Details |
|-------|--------|---------|
| Null values | PASS/FAIL | [Details] |
| Duplicates | PASS/FAIL | [Details] |
| Value ranges | PASS/FAIL | [Details] |

### Validation Results
| Test | Expected | Actual | Status |
|------|----------|--------|--------|
| Row count | X | Y | PASS/FAIL |
| Total sum | X | Y | PASS/FAIL |

### Issues Found
1. [Issue description and severity]
2. [Issue description and severity]

### Recommendations
- [Action to take]
\`\`\`

## Example Queries

### Check for nulls
\`\`\`sql
SELECT
    COUNT(*) AS total_rows,
    COUNT(column1) AS non_null_col1,
    COUNT(*) - COUNT(column1) AS null_col1
FROM table_name;
\`\`\`

### Check for duplicates
\`\`\`sql
SELECT
    id,
    COUNT(*) AS cnt
FROM table_name
GROUP BY id
HAVING COUNT(*) > 1;
\`\`\`

## Examples
- \`/test data quality in orders table\`
- \`/test this SQL query results\`
- \`/test the analysis results make sense\`
`,

    'doc.md': `# /doc - Generate Documentation

Document: $ARGUMENTS

## Purpose
Create clear documentation for analysis, code, or processes.

## Documentation Types

### 1. Analysis Documentation
- Executive summary
- Methodology
- Key findings
- Recommendations

### 2. Technical Documentation
- Code comments
- README files
- API documentation
- Data dictionaries

### 3. Process Documentation
- Step-by-step guides
- Runbooks
- FAQs

## Output Formats

### Analysis Report
\`\`\`markdown
# [Analysis Title]

## Executive Summary
[2-3 sentences summarizing key findings and recommendations]

## Background
[Context and business question]

## Methodology
[Data sources, approach, timeframe]

## Key Findings
### Finding 1
[Description with supporting data/visualization]

### Finding 2
[Description with supporting data/visualization]

## Recommendations
1. [Action item with expected impact]
2. [Action item with expected impact]

## Appendix
- Data sources
- SQL queries
- Assumptions
\`\`\`

### Data Dictionary
\`\`\`markdown
# Data Dictionary: [Table/Dataset Name]

## Overview
[Description of the dataset]

## Fields
| Field | Type | Description | Example |
|-------|------|-------------|---------|
| id | INT | Primary key | 12345 |
| created_at | TIMESTAMP | Record creation time | 2024-01-15 10:30:00 |

## Relationships
- Joins to [other_table] on [field]

## Notes
- [Important information about the data]
\`\`\`

## Examples
- \`/doc this SQL query\`
- \`/doc the analysis we just completed\`
- \`/doc data dictionary for users table\`
`,

    'commit.md': `# /commit - Git Commit

Commit changes: $ARGUMENTS

## Purpose
Create a well-formatted git commit with proper message.

## Process

### 1. Check Status
\`\`\`bash
git status
git diff --staged
\`\`\`

### 2. Stage Changes
\`\`\`bash
git add <files>
# or
git add -A
\`\`\`

### 3. Commit with Message
Follow conventional commit format:

\`\`\`
<type>(<scope>): <description>

[optional body]

[optional footer]
\`\`\`

## Commit Types
| Type | Description |
|------|-------------|
| feat | New feature |
| fix | Bug fix |
| docs | Documentation only |
| style | Formatting, no code change |
| refactor | Code restructuring |
| test | Adding tests |
| chore | Maintenance |
| data | Data changes (queries, notebooks) |
| analysis | Analysis work |

## Examples

### Simple commit
\`\`\`bash
git commit -m "feat(analysis): add monthly revenue query"
\`\`\`

### Detailed commit
\`\`\`bash
git commit -m "analysis(churn): complete customer churn analysis

- Added cohort analysis for Q1 2024
- Identified top 3 churn factors
- Created retention dashboard

Closes #123"
\`\`\`

## Best Practices
- Keep subject line under 50 characters
- Use imperative mood ("add" not "added")
- Reference issue numbers when applicable
- Separate subject from body with blank line

## Usage
- \`/commit\` - Auto-generate commit message from changes
- \`/commit add revenue analysis\` - Commit with custom message
`,

    // ============================================
    // DA COMMANDS (5)
    // ============================================

    'da/query.md': `# /da:query - Write SQL Query

Query for: $ARGUMENTS

## Purpose
Write optimized SQL queries based on business requirements.

## Process

### 1. Understand Requirements
- What question are we answering?
- What level of granularity?
- What time period?
- Any filters or segments?

### 2. Identify Tables
- What tables contain the data?
- How do they join?
- What are the key fields?

### 3. Write Query
- Use CTEs for readability
- Add comments
- Optimize for performance

### 4. Validate
- Check row counts
- Verify calculations
- Test edge cases

## Output Format
\`\`\`sql
-- ===========================================
-- Query: [Title]
-- Purpose: [Business question being answered]
-- Author: DA Toolkit
-- Date: [Current date]
-- ===========================================

-- Step 1: [Description]
WITH base AS (
    SELECT
        user_id,
        created_at,
        amount
    FROM orders
    WHERE created_at >= '2024-01-01'
),

-- Step 2: [Description]
aggregated AS (
    SELECT
        DATE_TRUNC('month', created_at) AS month,
        COUNT(DISTINCT user_id) AS unique_users,
        SUM(amount) AS total_revenue
    FROM base
    GROUP BY 1
)

-- Final output
SELECT
    month,
    unique_users,
    total_revenue,
    total_revenue / unique_users AS revenue_per_user
FROM aggregated
ORDER BY month;
\`\`\`

## Common Patterns

### Window Functions
\`\`\`sql
-- Running total
SUM(amount) OVER (ORDER BY date) AS running_total

-- Month-over-month change
LAG(value) OVER (ORDER BY month) AS prev_month

-- Rank within group
ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) AS rank
\`\`\`

### Date Operations
\`\`\`sql
-- PostgreSQL
DATE_TRUNC('month', created_at)
created_at::date

-- BigQuery
DATE_TRUNC(created_at, MONTH)
DATE(created_at)
\`\`\`

## Examples
- \`/da:query monthly active users by country\`
- \`/da:query customer lifetime value\`
- \`/da:query top 10 products by revenue\`
`,

    'da/analyze.md': `# /da:analyze - Exploratory Data Analysis

Analyze: $ARGUMENTS

## Purpose
Perform exploratory data analysis (EDA) to understand patterns and insights.

## EDA Framework

### 1. Data Overview
- Shape (rows, columns)
- Data types
- Missing values
- Sample records

### 2. Univariate Analysis
- Distributions
- Central tendency (mean, median, mode)
- Spread (std, IQR, range)
- Outliers

### 3. Bivariate Analysis
- Correlations
- Group comparisons
- Trend analysis

### 4. Key Insights
- Patterns discovered
- Anomalies found
- Hypotheses formed

## Output Format

### SQL Approach
\`\`\`sql
-- Data Overview
SELECT
    COUNT(*) AS total_rows,
    COUNT(DISTINCT user_id) AS unique_users,
    MIN(created_at) AS first_record,
    MAX(created_at) AS last_record
FROM table_name;

-- Distribution
SELECT
    segment,
    COUNT(*) AS count,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS percentage
FROM table_name
GROUP BY segment
ORDER BY count DESC;

-- Statistics
SELECT
    AVG(amount) AS mean,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median,
    STDDEV(amount) AS std_dev,
    MIN(amount) AS min_value,
    MAX(amount) AS max_value
FROM table_name;
\`\`\`

### Python Approach
\`\`\`python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_sql(query, connection)

# Overview
print(df.info())
print(df.describe())

# Distributions
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
df['amount'].hist(ax=axes[0,0])
df['category'].value_counts().plot(kind='bar', ax=axes[0,1])
sns.boxplot(data=df, x='segment', y='amount', ax=axes[1,0])
df.groupby('month')['amount'].sum().plot(ax=axes[1,1])
plt.tight_layout()
\`\`\`

## Analysis Types

| Type | When to Use | Key Metrics |
|------|-------------|-------------|
| Cohort | User behavior over time | Retention, LTV |
| Funnel | Conversion process | Drop-off rates |
| Segmentation | Group comparison | Segment sizes, differences |
| Time Series | Trends over time | Growth, seasonality |
| A/B Test | Experiment results | Significance, lift |

## Examples
- \`/da:analyze customer purchase patterns\`
- \`/da:analyze website funnel conversion\`
- \`/da:analyze revenue trends by segment\`
`,

    'da/report.md': `# /da:report - Generate Analysis Report

Report on: $ARGUMENTS

## Purpose
Create a comprehensive analysis report for stakeholders.

## Report Structure

### 1. Executive Summary
- Key findings (3-5 bullets)
- Main recommendations
- Bottom line

### 2. Background
- Business context
- Why this analysis
- Key questions

### 3. Methodology
- Data sources
- Time period
- Approach taken
- Limitations

### 4. Findings
- Detailed analysis
- Visualizations
- Supporting data

### 5. Recommendations
- Actionable items
- Expected impact
- Priority

### 6. Appendix
- Detailed tables
- SQL queries
- Technical notes

## Output Template

\`\`\`markdown
# [Report Title]
**Date:** [Date]
**Author:** Data Analyst
**Status:** Draft / Final

---

## Executive Summary

[2-3 paragraphs summarizing the most important findings and recommendations]

**Key Findings:**
- Finding 1: [One sentence with key metric]
- Finding 2: [One sentence with key metric]
- Finding 3: [One sentence with key metric]

**Recommendations:**
1. [Action] - Expected impact: [X%]
2. [Action] - Expected impact: [X%]

---

## Background

### Business Context
[Why this analysis was needed]

### Key Questions
1. [Question 1]
2. [Question 2]
3. [Question 3]

---

## Methodology

### Data Sources
| Source | Description | Time Period |
|--------|-------------|-------------|
| [Table] | [Description] | [Period] |

### Approach
[Description of analytical approach]

### Limitations
- [Limitation 1]
- [Limitation 2]

---

## Findings

### Finding 1: [Title]
[Detailed description with supporting data]

| Metric | Value | Change |
|--------|-------|--------|
| [Metric] | [Value] | [+/-X%] |

### Finding 2: [Title]
[Detailed description with supporting data]

---

## Recommendations

| Priority | Recommendation | Expected Impact | Effort |
|----------|----------------|-----------------|--------|
| 1 | [Action] | [Impact] | [H/M/L] |
| 2 | [Action] | [Impact] | [H/M/L] |

---

## Appendix

### A. SQL Queries
[Include key queries]

### B. Data Dictionary
[Relevant field definitions]
\`\`\`

## Examples
- \`/da:report Q1 revenue analysis\`
- \`/da:report customer churn investigation\`
- \`/da:report A/B test results for new feature\`
`,

    'da/dashboard.md': `# /da:dashboard - Design Dashboard

Dashboard for: $ARGUMENTS

## Purpose
Design a dashboard layout with metrics, charts, and filters.

## Dashboard Design Process

### 1. Define Audience
- Who will use this?
- What decisions will they make?
- How often will they view it?

### 2. Select Metrics
- What are the KPIs?
- What dimensions for breakdown?
- What comparisons needed?

### 3. Design Layout
- Visual hierarchy
- Information flow
- Interactive elements

### 4. Specify Charts
- Chart type for each metric
- Filters and drill-downs
- Refresh frequency

## Output Template

\`\`\`markdown
# Dashboard Specification: [Name]

## Overview
- **Purpose:** [What decisions this enables]
- **Audience:** [Who will use it]
- **Refresh:** [Real-time / Daily / Weekly]

## Layout

\`\`\`
+------------------+------------------+------------------+
|   KPI Card 1     |   KPI Card 2     |   KPI Card 3     |
|   [Metric]       |   [Metric]       |   [Metric]       |
+------------------+------------------+------------------+
|                                                        |
|              Main Chart: [Type]                        |
|              [Metric over time]                        |
|                                                        |
+-------------------------+------------------------------+
|     Chart 2             |     Chart 3                  |
|     [Type]              |     [Type]                   |
|     [Breakdown]         |     [Breakdown]              |
+-------------------------+------------------------------+
|                    Data Table                          |
|     [Detail rows with key dimensions]                  |
+--------------------------------------------------------+
\`\`\`

## Metrics

| Metric | Calculation | Chart Type |
|--------|-------------|------------|
| [KPI 1] | [SQL/Formula] | Scorecard |
| [KPI 2] | [SQL/Formula] | Scorecard |
| [Trend] | [SQL/Formula] | Line chart |
| [Breakdown] | [SQL/Formula] | Bar chart |

## Filters
- Date range (default: Last 30 days)
- [Dimension 1] (multi-select)
- [Dimension 2] (single-select)

## SQL Queries

### KPI 1: [Name]
\`\`\`sql
SELECT
    COUNT(DISTINCT user_id) AS value,
    'users' AS label
FROM events
WHERE event_date >= CURRENT_DATE - 30;
\`\`\`

### Main Chart: [Name]
\`\`\`sql
SELECT
    DATE_TRUNC('day', event_date) AS date,
    COUNT(*) AS events
FROM events
WHERE event_date >= CURRENT_DATE - 30
GROUP BY 1
ORDER BY 1;
\`\`\`

## Interactivity
- Click on chart → Filter table below
- Hover → Show tooltip with details
- Click KPI → Drill down to detail view

## Implementation Notes
- Tool: [Superset / Metabase / Looker / etc.]
- Data source: [Database/table]
- Cache: [Duration]
\`\`\`

## Chart Type Guidelines

| Data Type | Recommended Chart |
|-----------|-------------------|
| Trend over time | Line chart |
| Category comparison | Bar chart |
| Part of whole | Pie/Donut (≤5 categories) |
| Distribution | Histogram |
| Correlation | Scatter plot |
| Geographic | Map |
| Single value | Scorecard/KPI card |

## Examples
- \`/da:dashboard executive revenue overview\`
- \`/da:dashboard marketing campaign performance\`
- \`/da:dashboard customer support metrics\`
`,

    'da/notebook.md': `# /da:notebook - Create Jupyter Notebook

Notebook for: $ARGUMENTS

## Purpose
Create a structured Jupyter notebook for analysis or data processing.

## Notebook Structure

### Standard Sections
1. **Title & Overview** - What this notebook does
2. **Setup** - Imports and configuration
3. **Data Loading** - Get the data
4. **Exploration** - Understand the data
5. **Analysis** - Main analysis work
6. **Visualization** - Charts and graphs
7. **Conclusions** - Summary and next steps

## Output Template

\`\`\`python
# %% [markdown]
# # [Analysis Title]
#
# **Author:** Data Analyst
# **Date:** [Date]
# **Purpose:** [One-line description]
#
# ## Overview
# [2-3 sentences about what this notebook accomplishes]

# %% [markdown]
# ## 1. Setup

# %%
# Standard imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# Configuration
pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 100)
plt.style.use('seaborn-v0_8-whitegrid')
%matplotlib inline

# Database connection (if needed)
# from sqlalchemy import create_engine
# engine = create_engine('postgresql://...')

# %% [markdown]
# ## 2. Data Loading

# %%
# Option 1: From SQL
query = """
SELECT *
FROM table_name
WHERE date >= '2024-01-01'
"""
# df = pd.read_sql(query, engine)

# Option 2: From file
# df = pd.read_csv('data.csv')

# Option 3: Sample data for development
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=100),
    'user_id': np.random.randint(1, 1000, 100),
    'amount': np.random.uniform(10, 500, 100),
    'category': np.random.choice(['A', 'B', 'C'], 100)
})

print(f"Loaded {len(df):,} rows")
df.head()

# %% [markdown]
# ## 3. Data Exploration

# %%
# Basic info
print("Shape:", df.shape)
print("\\nData types:")
print(df.dtypes)
print("\\nMissing values:")
print(df.isnull().sum())

# %%
# Summary statistics
df.describe()

# %%
# Value distributions
for col in df.select_dtypes(include=['object', 'category']).columns:
    print(f"\\n{col}:")
    print(df[col].value_counts())

# %% [markdown]
# ## 4. Analysis

# %%
# [Main analysis code here]
# Group by analysis
summary = df.groupby('category').agg({
    'amount': ['sum', 'mean', 'count'],
    'user_id': 'nunique'
}).round(2)
summary.columns = ['total_amount', 'avg_amount', 'transactions', 'unique_users']
summary

# %%
# Time series analysis
daily = df.groupby('date')['amount'].sum().reset_index()
daily['rolling_7d'] = daily['amount'].rolling(7).mean()
daily

# %% [markdown]
# ## 5. Visualization

# %%
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Chart 1: Distribution
df['amount'].hist(bins=30, ax=axes[0,0], edgecolor='black')
axes[0,0].set_title('Amount Distribution')
axes[0,0].set_xlabel('Amount')

# Chart 2: Category breakdown
df.groupby('category')['amount'].sum().plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('Total by Category')
axes[0,1].set_xlabel('Category')

# Chart 3: Time series
axes[1,0].plot(daily['date'], daily['amount'], alpha=0.5, label='Daily')
axes[1,0].plot(daily['date'], daily['rolling_7d'], label='7-day avg')
axes[1,0].set_title('Amount Over Time')
axes[1,0].legend()

# Chart 4: Box plot
df.boxplot(column='amount', by='category', ax=axes[1,1])
axes[1,1].set_title('Amount by Category')

plt.tight_layout()
plt.savefig('analysis_charts.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## 6. Conclusions
#
# ### Key Findings
# 1. [Finding 1]
# 2. [Finding 2]
# 3. [Finding 3]
#
# ### Recommendations
# - [Recommendation 1]
# - [Recommendation 2]
#
# ### Next Steps
# - [ ] [Follow-up analysis]
# - [ ] [Data to collect]

# %%
# Export results if needed
# summary.to_csv('results.csv', index=False)
\`\`\`

## Notebook Best Practices

1. **Clear structure** - Use markdown headers to organize
2. **Reproducible** - Include all imports and data loading
3. **Documented** - Explain what each section does
4. **Visualized** - Use charts to support findings
5. **Actionable** - End with conclusions and next steps

## Examples
- \`/da:notebook customer churn analysis\`
- \`/da:notebook A/B test results evaluation\`
- \`/da:notebook monthly metrics review\`
`,
  };
}
